# Spark local cluster using Docker
#
# Note: from CIMReader/src/test/resources/ directory run with:
#       docker-compose up&
# stop with:
#       docker-compose down
#
# Note: for more worker nodes use:
#       docker-compose up --scale worker=2&
#
# Note: to start a shell on the cluster use:
#       docker exec --interactive --tty spark_master bash
# or    docker exec --interactive --tty spark_worker_1 bash
#

version: "2" # Docker Engine (version > 1.10.0) and Docker Compose (version >= 1.6.0)

services:
  sandbox:
    container_name: spark_master
    image: derrickoswald/spark-docker
    command: start-spark master
    environment:
      HDFS_USER: ${USER} # use value of environment variable "USER" as Spark owner/operator
    hostname: sandbox
    ports:
      - "4040:4040" # Cluster Manager Web UI
      - "6066:6066" # Standalone Master REST port (spark.master.rest.port)
      - "7077:7077" # Driver to Standalone Master, as in master = spark://sandbox:7077
      - "8020:8020" # DFS Namenode IPC, as in: hdfs dfs -fs hdfs://sandbox:8020 -ls
      - "8080:8080" # Standalone Master Web UI
      - "8081:8081" # Standalone Worker Web UI
      - "10000:10000" # Thriftserver JDBC port
      - "10001:10001" # Thriftserver HTTP protocol JDBC port
      - "50010:50010" # DFS Datanode data transfer
      - "50070:50070" # DFS Namenode Web UI
      - "50075:50075" # DFS Datanode Web UI
      - "7000:7000" # Cassandra storage_port
      - "7001:7001" # Cassandra ssl_storage_port
      - "9042:9042" # Cassandra native_transport_port
      - "9160:9160" # Cassandra rpc_port
    volumes:
      - ./../../../data:/opt/data # map sample data file directory to /opt/data within the container
      - ./../../../private_data:/opt/private_data # map private sample data file directory to /opt/private_data within the container
      - ./../../../target:/opt/code # map compilation output directory to /opt/code within the container
  worker:
    image: derrickoswald/spark-docker
    command: start-spark worker sandbox
    environment:
      HDFS_USER: ${USER} # use value of environment variable "USER" as Spark owner/operator
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 4g
    links:
      - sandbox
    volumes:
      - ./../../../data:/opt/data # map sample data file directory to /opt/data within the container
      - ./../../../private_data:/opt/private_data # map private sample data file directory to /opt/private_data within the container
      - ./../../../target:/opt/code # map compilation output directory to /opt/code within the container
